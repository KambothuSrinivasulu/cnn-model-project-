{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"techsash/waste-classification-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Suppress warnings for clarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train_path = \"/root/.cache/kagglehub/datasets/techsash/waste-classification-data/versions/1/DATASET/TRAIN\"\n",
        "test_path = \"/root/.cache/kagglehub/datasets/techsash/waste-classification-data/versions/1/DATASET/TEST\"\n",
        "\n",
        "# Define data generators first\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(224, 224), # Removed the extra dimension (3) as it's inferred\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(224, 224), # Removed the extra dimension (3) as it's inferred\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=True\n",
        ")\n",
        "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(224, 224), # Removed the extra dimension (3) as it's inferred\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "# Now define and compile the model\n",
        "def build_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)), # Corrected input shape\n",
        "        MaxPooling2D(2, 2),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(len(train_generator.class_indices), activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.keras', save_best_only=True)\n",
        "]\n",
        "\n",
        "# Train the CNN model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=25,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VqdtBdISIuU",
        "outputId": "3e6996da-0de3-4f3f-b227-c55f214e9681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/techsash/waste-classification-data?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 427M/427M [00:03<00:00, 123MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/techsash/waste-classification-data/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cv2 import cvtColor\n",
        "x_data = []\n",
        "y_data = []\n",
        "for category in glob(train_path+'/*'):\n",
        "    for file in tqdm(glob(category+'/*')):\n",
        "        img_array = cv2.imread(file)\n",
        "        img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
        "        x_data.append(img_array)\n",
        "        y_data.append(category.split(\"/\")[-1])\n",
        "data = pd.DataFrame({'image': x_data, 'label': y_data})"
      ],
      "metadata": {
        "id": "q-nOtkWDa88z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['#a0d157', '#c48bb8']\n",
        "plt.pie(data.label.value_counts(), labels=['Organic', 'Recyclable'], autopct='%0.2f%%',\n",
        "        colors=colors, startangle=90, explode=[0.05, 0.05])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RkGh-CweahnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save(\"Waste-Classification-CNN-Model.h5\")"
      ],
      "metadata": {
        "id": "-6tyd1O3Wlfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e8yKrQHkW2ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the best saved model\n",
        "model = load_model(\"Waste-Classification-CNN-Model.h5\")\n",
        "\n",
        "# Display the model architecture again to verify loading\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "iL6nRvpoXBqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "XcahmtU2ZAsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path =  \"/root/.cache/kagglehub/datasets/techsash/waste-classification-data/versions/1/DATASET/TEST\"\n",
        "\n",
        "# Rescale test images to match training data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load test dataset with similar preprocessing\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,  # Do not shuffle so that labels remain aligned\n",
        "    # **Add this line to handle potential empty directories:**\n",
        "    # classes=['O', 'R']  # Replace with your actual class names #Commented out classes parameter\n",
        "    # Instead of specifying classes, try to infer them automatically:\n",
        "    classes=None # This will infer classes from subdirectory names\n",
        ")\n",
        "\n",
        "# Check if the test directory or subdirectories are empty\n",
        "if test_generator.samples == 0:\n",
        "    print(\"Warning: Test directory is empty or has no valid images. Evaluation might fail.\")\n",
        "    # If the directory is empty, raise an exception to stop execution:\n",
        "    raise ValueError(\"Test directory is empty or has no valid images.\")\n",
        "\n",
        "# Evaluate the model\n",
        "results = model.evaluate(test_generator, verbose=1, return_dict=True)\n",
        "\n",
        "# Check if evaluation results are valid\n",
        "if results is not None:\n",
        "    test_loss, test_accuracy = results['loss'], results['accuracy']\n",
        "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "else:\n",
        "    print(\"Error: Model evaluation failed. Check your test data and generator settings.\")"
      ],
      "metadata": {
        "id": "7XSwH-Q-aUhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions (probabilities)\n",
        "predictions = model.predict(test_generator)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get actual class labels from test generator\n",
        "actual_classes = test_generator.classes\n",
        "\n",
        "# Get class labels (to map indices back to names)\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Print sample predictions\n",
        "print(\"Predicted Classes:\", [class_labels[i] for i in predicted_classes[:10]])\n",
        "print(\"Actual Classes:\", [class_labels[i] for i in actual_classes[:10]])"
      ],
      "metadata": {
        "id": "BYKeQ9khZtP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get file names for test images\n",
        "test_image_paths = test_generator.filepaths\n",
        "\n",
        "# Select random indices\n",
        "random_indices = np.random.choice(len(test_image_paths), 6, replace=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i, idx in enumerate(random_indices):\n",
        "    img = cv2.imread(test_image_paths[idx])\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for correct color display\n",
        "\n",
        "    # Get predicted label\n",
        "    predicted_label = class_labels[predicted_classes[idx]]\n",
        "    actual_label = class_labels[actual_classes[idx]]\n",
        "\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predicted: {predicted_label}\\nActual: {actual_label}\", fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5na08oDnZ2fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(actual_classes, predicted_classes, target_names=class_labels))\n",
        "\n",
        "# Generate and plot confusion matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(actual_classes, predicted_classes)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r7tuKBMlZ-JW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}